{% set name = "llama-cpp-python" %}
# NOTE: VERIFY llama_cpp_version before merging!
{% set version = "0.2.24" %}
{% set llama_cpp_version = "0.0.1660" %}

package:
  name: {{ name|lower }}
  version: {{ version }}

source:
  url: https://pypi.io/packages/source/{{ name[0] }}/{{ name }}/llama_cpp_python-{{ version }}.tar.gz
  sha256: 85f8fd110b4b90599d5ff427bd4a1a4db6e70817c60ba8aa609fa5c645761ec1
  patches:
    # Asks cdll to look for the library in the path as well.
    - try-lib-in-path.patch

build:
  number: 0
  script:
    {% macro cmake_args(key, value) -%}
    - export CMAKE_ARGS="${CMAKE_ARGS} {{ key }}={{ value }}"    # [unix]
    - set CMAKE_ARGS=%CMAKE_ARGS% {{ key }}={{ value }}          # [win]
    {%- endmacro %}

    {{ cmake_args("-DLLAMA_BUILD", "OFF") }}
    {{ cmake_args("-DLLAVA_BUILD", "OFF") }}

    - {{ PYTHON }} -m pip install . -vv
requirements:
  build:
    - python                                 # [build_platform != target_platform]
    - cross-python_{{ target_platform }}     # [build_platform != target_platform]

    - {{ compiler('c') }}
    - {{ compiler('cxx') }}
    - cmake
    - make
    - pkgconfig

  host:
    - python
    - scikit-build-core >=0.5.1
    - pip

  run:
    - python
    - typing-extensions >=4.5.0
    - numpy >=1.20.0
    - diskcache >=5.6.1

    - llama.cpp {{ llama_cpp_version }}

    # Split into llama-cpp-python-server
    - uvicorn >=0.22.0
    - fastapi >=0.100.0
    - pydantic-settings >=2.0.1
    - sse-starlette >=1.6.1
    - starlette-context >=0.3.6,<0.4
test:
  imports:
    - llama_cpp
  commands:
    - pip check
  requires:
    - pip

about:
  home: https://github.com/abetlen/llama-cpp-python
  summary: Python bindings for the llama.cpp library
  license: MIT
  license_file:
    - LICENSE.md

extra:
  recipe-maintainers:
    - YYYasin19
    - sodre
